{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyNff3ecNvAOmymHSuuKJ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lastinm/ml_hw_notebooks/blob/main/%D0%A5%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5%2C_%D0%B8%D0%B7%D0%B2%D0%BB%D0%B5%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%D0%B8_%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%91%D0%94_%D0%9B%D0%B0%D0%B1%D0%BE%D1%80%D0%B0%D1%82%D0%BE%D1%80%D0%BD%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_%E2%84%965.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1. Structured Streaming из набора файлов"
      ],
      "metadata": {
        "id": "nw-PCLkaqy5R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RsnUJj9gjQvU"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "  .builder \\\n",
        "  .appName(\"Streaming 1\") \\\n",
        "  .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
        "  .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Подготовить данные, имеющиеся во вложении.\n",
        "2. Открыть на основе исходных json файлов потоковый датафрейм."
      ],
      "metadata": {
        "id": "zpvJHH-3q5WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.readStream \\\n",
        "  .format(\"json\") \\\n",
        "  .option(\"path\", \"/content/sample_data\") \\\n",
        "  .option(\"maxFilesPerTrigger\", 1) \\\n",
        "  .load()"
      ],
      "metadata": {
        "id": "eTBrXZ0Gj--F"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Написать преобразование исходного датафрейма, для получения плоской (flat) структуры."
      ],
      "metadata": {
        "id": "VOUAt1PZrDNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.selectExpr(\"InvoiceNumber\",\n",
        "                    \"CreatedTime\",\n",
        "                    \"StoreID\",\n",
        "                    \"PosID\",\n",
        "                    \"CashierID\",\n",
        "                    \"CustomerType\",\n",
        "                    \"CustomerCardNo\",\n",
        "                    \"TotalAmount\",\n",
        "                    \"NumberOfItems\",\n",
        "                    \"PaymentMethod\",\n",
        "                    \"TaxableAmount\",\n",
        "                    \"explode(InvoiceLineItems) as Item\")"
      ],
      "metadata": {
        "id": "RSnhBDkXmTSZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# [{\"ItemCode\":\"458\",\"ItemDescription\":\"Wine glass\",\"ItemPrice\":1644.0,\"ItemQty\":2,\"TotalValue\":3288.0},{\"ItemCode\":\"283\",\"ItemDescription\":\"Portable Lamps\",\"ItemPrice\":2236.0,\"ItemQty\":1,\"TotalValue\":2236.0},{\"ItemCode\":\"498\",\"ItemDescription\":\"Carving knifes\",\"ItemPrice\":1424.0,\"ItemQty\":2,\"TotalValue\":2848.0},{\"ItemCode\":\"523\",\"ItemDescription\":\"Oil-lamp clock\",\"ItemPrice\":1371.0,\"ItemQty\":2,\"TotalValue\":2742.0}]\n",
        "df3 = df2 \\\n",
        "  .withColumn(\"ItemCode\", expr(\"Item.ItemCode\")) \\\n",
        "  .withColumn(\"ItemDescription\", expr(\"Item.ItemDescription\")) \\\n",
        "  .withColumn(\"ItemPrice\", expr(\"Item.ItemPrice\")) \\\n",
        "  .withColumn(\"ItemQty\", expr(\"Item.ItemQty\")) \\\n",
        "  .withColumn(\"TotalValue\", expr(\"Item.TotalValue\")) \\\n",
        "  .drop(\"Item\")"
      ],
      "metadata": {
        "id": "O5vY-QoVn-AY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Сохранять получаемые каждые X секунд исходные файлы в json."
      ],
      "metadata": {
        "id": "K0AnqNQhrIR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = df3 \\\n",
        "  .writeStream \\\n",
        "  .format(\"json\") \\\n",
        "  .option(\"path\", \"/content/output\") \\\n",
        "  .option(\"checkpointLocation\", \"/content/checkpoint\") \\\n",
        "  .trigger(processingTime=\"10 seconds\") \\\n",
        "  .start()"
      ],
      "metadata": {
        "id": "nrt-QYLZpRxT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.lastProgress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPT-YVP7qWYt",
        "outputId": "0c744bf2-faa0-453f-c844-37708a93cf36"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'b5c88650-07d1-4a2f-80bc-4787b726564a',\n",
              " 'runId': '36e98a25-8bab-4937-b15d-d2283405e630',\n",
              " 'name': None,\n",
              " 'timestamp': '2025-01-04T18:53:40.000Z',\n",
              " 'batchId': 9,\n",
              " 'numInputRows': 0,\n",
              " 'inputRowsPerSecond': 0.0,\n",
              " 'processedRowsPerSecond': 0.0,\n",
              " 'durationMs': {'latestOffset': 4, 'triggerExecution': 6},\n",
              " 'stateOperators': [],\n",
              " 'sources': [{'description': 'FileStreamSource[file:/content/sample_data]',\n",
              "   'startOffset': {'logOffset': 8},\n",
              "   'endOffset': {'logOffset': 8},\n",
              "   'latestOffset': None,\n",
              "   'numInputRows': 0,\n",
              "   'inputRowsPerSecond': 0.0,\n",
              "   'processedRowsPerSecond': 0.0}],\n",
              " 'sink': {'description': 'FileSink[/content/output]', 'numOutputRows': -1}}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "hdSuPpd-kX5J"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2. Применение Tumbling Window"
      ],
      "metadata": {
        "id": "B5tTLAkOsKrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Подготовить подходящие данные или воспользоваться продемонстрированным на паре генератором."
      ],
      "metadata": {
        "id": "aWUiko_8tvMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession\\\n",
        "  .builder \\\n",
        "  .appName(\"Streaming 2\") \\\n",
        "  .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
        "  .getOrCreate()"
      ],
      "metadata": {
        "id": "8O0t0_Dqtq9B"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Открыть на основе исходных json файлов потоковый датафрейм."
      ],
      "metadata": {
        "id": "R4Bq_i9Qt0oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Определим структуру будущего датафрейма\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"createdTime\", StringType()),\n",
        "    StructField(\"price\", DoubleType()),\n",
        "    StructField(\"quantity\", IntegerType()),\n",
        "    StructField(\"type\", StringType())\n",
        "])"
      ],
      "metadata": {
        "id": "z_lVDtp7uCfc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем датафрейм\n",
        "df = spark.readStream \\\n",
        "    .format(\"json\") \\\n",
        "    .option(\"path\", \"/content/input2\") \\\n",
        "    .option(\"maxFilesPerTrigger\", 1) \\\n",
        "    .schema(schema) \\\n",
        "    .load()"
      ],
      "metadata": {
        "id": "kFEv4ea_uKqd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Генерация данных\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "\n",
        "\n",
        "# создаем от 5 до 10 файлов с небольшим количеством объектов\n",
        "# в каждом файле сдвигаем временную метку на 4 сек\n",
        "for frame in range(random.randint(5, 20)):\n",
        "  with open(f\"input2/{frame}.json\", \"w\") as file:\n",
        "    for _ in range(random.randint(10, 20)):\n",
        "      file.write(json.dumps({\n",
        "        \"createdTime\": int(time.time()) + 4 * frame,\n",
        "        \"price\": random.randint(1000, 10_000) / 100,\n",
        "        \"quantity\":random.randint(100, 2000),\n",
        "        \"type\": random.choice(['type1', 'type2', 'type3'])\n",
        "      }))\n",
        "      file.write(\"\\n\")"
      ],
      "metadata": {
        "id": "jvy59kPTvzFs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Исходная схема: createdTime - string, а должен быть timestamp\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H430epJAv5wF",
        "outputId": "73a175e9-d513-41b3-bfa9-ada647f69ea4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- createdTime: string (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_timestamp, col, current_timestamp, from_unixtime\n",
        "df_2 = df\\\n",
        "  .withColumn(\"createdTime\", to_timestamp(from_unixtime(col(\"createdTime\"))))"
      ],
      "metadata": {
        "id": "PZOBpJxswBJu"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61l4DzkKwDpG",
        "outputId": "3460eae1-6485-418d-aa17-516729e79ac4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- createdTime: timestamp (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Написать произвольный запрос агрегации, используя функцию tumbling window."
      ],
      "metadata": {
        "id": "JM49jH0wxrJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем агрегацию с оконной функцией, к\n",
        "# оторая разбивает наши данные на равные отрезки - по 5 секунд,\n",
        "# в рамках которых и производятся агрегации.\n",
        "from pyspark.sql.functions import window, col, avg, max\n",
        "\n",
        "\n",
        "\n",
        "agg_df = df_2 \\\n",
        "    .groupBy(\n",
        "        window(col(\"createdTime\"), \"10 seconds\")\n",
        "    ) \\\n",
        "    .agg(\n",
        "      avg(\"price\").alias(\"avg_price\"),\n",
        "      max(\"quantity\").alias(\"max_quantity\"))"
      ],
      "metadata": {
        "id": "7qwItv58wOIP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = agg_df.select(\"window.start\", \"window.end\", \"avg_price\", \"max_quantity\")"
      ],
      "metadata": {
        "id": "mrE8NQ5AwWBp"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-j8pRvFwYDh",
        "outputId": "76d36925-e206-4af4-faff-53a31253c12a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- start: timestamp (nullable = true)\n",
            " |-- end: timestamp (nullable = true)\n",
            " |-- avg_price: double (nullable = true)\n",
            " |-- max_quantity: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Сохранять получаемые каждые X секунд исходные файлы в json."
      ],
      "metadata": {
        "id": "gL91tSKVx9WP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Опция `.outputMode(\"complete\")` указыает на то,\n",
        "# что в результате получаем один df в котором будут актуальные данные (обновляются все строки)\n",
        "query = result_df.writeStream \\\n",
        "  .format(\"memory\") \\\n",
        "  .outputMode(\"complete\") \\\n",
        "  .option(\"queryName\", \"second_query2\") \\\n",
        "  .option(\"checkpointLocation\", \"check-dir-6\") \\\n",
        "  .trigger(processingTime = \"5 seconds\") \\\n",
        "  .start()"
      ],
      "metadata": {
        "id": "GdJi9EVBwbsh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.lastProgress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXk8sEvYwdiq",
        "outputId": "f53d28c4-61c4-4331-9e6e-5e05b55dfaf0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'c9681f18-f6aa-4f90-99be-5ccf757a17d0',\n",
              " 'runId': '1cfd9802-d341-487e-8789-412848478856',\n",
              " 'name': 'second_query2',\n",
              " 'timestamp': '2025-01-04T19:19:39.188Z',\n",
              " 'batchId': 0,\n",
              " 'numInputRows': 16,\n",
              " 'inputRowsPerSecond': 0.0,\n",
              " 'processedRowsPerSecond': 0.5708169818052087,\n",
              " 'durationMs': {'addBatch': 27493,\n",
              "  'commitOffsets': 101,\n",
              "  'getBatch': 20,\n",
              "  'latestOffset': 76,\n",
              "  'queryPlanning': 281,\n",
              "  'triggerExecution': 28030,\n",
              "  'walCommit': 48},\n",
              " 'stateOperators': [{'operatorName': 'stateStoreSave',\n",
              "   'numRowsTotal': 1,\n",
              "   'numRowsUpdated': 1,\n",
              "   'allUpdatesTimeMs': 1474,\n",
              "   'numRowsRemoved': 0,\n",
              "   'allRemovalsTimeMs': 0,\n",
              "   'commitTimeMs': 29578,\n",
              "   'memoryUsedBytes': 45096,\n",
              "   'numRowsDroppedByWatermark': 0,\n",
              "   'numShufflePartitions': 200,\n",
              "   'numStateStoreInstances': 200,\n",
              "   'customMetrics': {'loadedMapCacheHitCount': 0,\n",
              "    'loadedMapCacheMissCount': 0,\n",
              "    'stateOnCurrentVersionSizeBytes': 16296}}],\n",
              " 'sources': [{'description': 'FileStreamSource[file:/content/input2]',\n",
              "   'startOffset': None,\n",
              "   'endOffset': {'logOffset': 0},\n",
              "   'latestOffset': None,\n",
              "   'numInputRows': 16,\n",
              "   'inputRowsPerSecond': 0.0,\n",
              "   'processedRowsPerSecond': 0.5708169818052087}],\n",
              " 'sink': {'description': 'MemorySink', 'numOutputRows': 1}}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM second_query2\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHR-7B_ywzI1",
        "outputId": "59733bae-c7dd-4769-af1b-27063291e8f7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+-----------------+------------+\n",
            "|              start|                end|        avg_price|max_quantity|\n",
            "+-------------------+-------------------+-----------------+------------+\n",
            "|2025-01-04 19:16:30|2025-01-04 19:16:40|56.81321428571429|        1994|\n",
            "+-------------------+-------------------+-----------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Запись в файл после кадждого обновления (после каждого нового считанного batch)\n",
        "currentBatch = 0\n",
        "while not query.awaitTermination(5):\n",
        "  lastProgress = query.lastProgress\n",
        "  print(lastProgress)\n",
        "  if lastProgress['numInputRows'] == 0:\n",
        "    query.stop()\n",
        "  else:\n",
        "    batchId = lastProgress['batchId']\n",
        "    if currentBatch <= batchId:\n",
        "      spark.sql(\"SELECT * FROM second_query2\").write.json(f\"output2/{batchId}.json\")\n",
        "      currentBatch = batchId + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa55pG8ow2It",
        "outputId": "790f6b93-390e-4e0b-9912-cb2e91739770"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'c9681f18-f6aa-4f90-99be-5ccf757a17d0', 'runId': '1cfd9802-d341-487e-8789-412848478856', 'name': 'second_query2', 'timestamp': '2025-01-04T19:20:44.453Z', 'batchId': 3, 'numInputRows': 18, 'inputRowsPerSecond': 0.9688357823348942, 'processedRowsPerSecond': 0.9986684420772303, 'durationMs': {'addBatch': 17821, 'commitOffsets': 40, 'getBatch': 14, 'latestOffset': 64, 'queryPlanning': 41, 'triggerExecution': 18024, 'walCommit': 44}, 'stateOperators': [{'operatorName': 'stateStoreSave', 'numRowsTotal': 2, 'numRowsUpdated': 1, 'allUpdatesTimeMs': 762, 'numRowsRemoved': 0, 'allRemovalsTimeMs': 0, 'commitTimeMs': 24479, 'memoryUsedBytes': 86864, 'numRowsDroppedByWatermark': 0, 'numShufflePartitions': 200, 'numStateStoreInstances': 200, 'customMetrics': {'loadedMapCacheHitCount': 1200, 'loadedMapCacheMissCount': 0, 'stateOnCurrentVersionSizeBytes': 21232}}], 'sources': [{'description': 'FileStreamSource[file:/content/input2]', 'startOffset': {'logOffset': 2}, 'endOffset': {'logOffset': 3}, 'latestOffset': None, 'numInputRows': 18, 'inputRowsPerSecond': 0.9688357823348942, 'processedRowsPerSecond': 0.9986684420772303}], 'sink': {'description': 'MemorySink', 'numOutputRows': 2}}\n",
            "{'id': 'c9681f18-f6aa-4f90-99be-5ccf757a17d0', 'runId': '1cfd9802-d341-487e-8789-412848478856', 'name': 'second_query2', 'timestamp': '2025-01-04T19:21:02.480Z', 'batchId': 4, 'numInputRows': 15, 'inputRowsPerSecond': 0.8320852055250457, 'processedRowsPerSecond': 1.0178462373617425, 'durationMs': {'addBatch': 14470, 'commitOffsets': 133, 'getBatch': 9, 'latestOffset': 28, 'queryPlanning': 66, 'triggerExecution': 14737, 'walCommit': 29}, 'stateOperators': [{'operatorName': 'stateStoreSave', 'numRowsTotal': 2, 'numRowsUpdated': 1, 'allUpdatesTimeMs': 767, 'numRowsRemoved': 0, 'allRemovalsTimeMs': 0, 'commitTimeMs': 19671, 'memoryUsedBytes': 86984, 'numRowsDroppedByWatermark': 0, 'numShufflePartitions': 200, 'numStateStoreInstances': 200, 'customMetrics': {'loadedMapCacheHitCount': 1600, 'loadedMapCacheMissCount': 0, 'stateOnCurrentVersionSizeBytes': 21232}}], 'sources': [{'description': 'FileStreamSource[file:/content/input2]', 'startOffset': {'logOffset': 3}, 'endOffset': {'logOffset': 4}, 'latestOffset': None, 'numInputRows': 15, 'inputRowsPerSecond': 0.8320852055250457, 'processedRowsPerSecond': 1.0178462373617425}], 'sink': {'description': 'MemorySink', 'numOutputRows': 2}}\n",
            "{'id': 'c9681f18-f6aa-4f90-99be-5ccf757a17d0', 'runId': '1cfd9802-d341-487e-8789-412848478856', 'name': 'second_query2', 'timestamp': '2025-01-04T19:21:02.480Z', 'batchId': 4, 'numInputRows': 15, 'inputRowsPerSecond': 0.8320852055250457, 'processedRowsPerSecond': 1.0178462373617425, 'durationMs': {'addBatch': 14470, 'commitOffsets': 133, 'getBatch': 9, 'latestOffset': 28, 'queryPlanning': 66, 'triggerExecution': 14737, 'walCommit': 29}, 'stateOperators': [{'operatorName': 'stateStoreSave', 'numRowsTotal': 2, 'numRowsUpdated': 1, 'allUpdatesTimeMs': 767, 'numRowsRemoved': 0, 'allRemovalsTimeMs': 0, 'commitTimeMs': 19671, 'memoryUsedBytes': 86984, 'numRowsDroppedByWatermark': 0, 'numShufflePartitions': 200, 'numStateStoreInstances': 200, 'customMetrics': {'loadedMapCacheHitCount': 1600, 'loadedMapCacheMissCount': 0, 'stateOnCurrentVersionSizeBytes': 21232}}], 'sources': [{'description': 'FileStreamSource[file:/content/input2]', 'startOffset': {'logOffset': 3}, 'endOffset': {'logOffset': 4}, 'latestOffset': None, 'numInputRows': 15, 'inputRowsPerSecond': 0.8320852055250457, 'processedRowsPerSecond': 1.0178462373617425}], 'sink': {'description': 'MemorySink', 'numOutputRows': 2}}\n",
            "{'id': 'c9681f18-f6aa-4f90-99be-5ccf757a17d0', 'runId': '1cfd9802-d341-487e-8789-412848478856', 'name': 'second_query2', 'timestamp': '2025-01-04T19:21:30.000Z', 'batchId': 5, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 4, 'triggerExecution': 5}, 'stateOperators': [{'operatorName': 'stateStoreSave', 'numRowsTotal': 2, 'numRowsUpdated': 0, 'allUpdatesTimeMs': 767, 'numRowsRemoved': 0, 'allRemovalsTimeMs': 0, 'commitTimeMs': 19671, 'memoryUsedBytes': 86984, 'numRowsDroppedByWatermark': 0, 'numShufflePartitions': 200, 'numStateStoreInstances': 200, 'customMetrics': {'loadedMapCacheHitCount': 1600, 'loadedMapCacheMissCount': 0, 'stateOnCurrentVersionSizeBytes': 21232}}], 'sources': [{'description': 'FileStreamSource[file:/content/input2]', 'startOffset': {'logOffset': 4}, 'endOffset': {'logOffset': 4}, 'latestOffset': None, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0}], 'sink': {'description': 'MemorySink', 'numOutputRows': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Запись в файл после считывания всех файлов (легче и предпочтительнее)\n",
        "currentBatch = 0\n",
        "while not query.awaitTermination(5):\n",
        "  lastProgress = query.lastProgress\n",
        "  print(lastProgress)\n",
        "  if lastProgress['numInputRows'] == 0:\n",
        "    spark.sql(\"SELECT * FROM second_query2\").write.json(f\"output2/{batchId}.json\")\n",
        "    query.stop()"
      ],
      "metadata": {
        "id": "tBDjSv9Hw8Ke"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/check-dir-6/\n",
        "!rm -r /content/input2/\n",
        "!mkdir /content/input2"
      ],
      "metadata": {
        "id": "JBqDz_NFxDCv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "5XPhnvNQxH2Q"
      },
      "execution_count": 49,
      "outputs": []
    }
  ]
}